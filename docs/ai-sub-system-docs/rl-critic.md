# RL Critic

_Version 1.0 — last updated 2025‑06‑20_

The RL‑Critic subsystem scores alternative tech‑stack plans generated by
Fast Downward / OPTIC and selects the one that minimises risk, build time,
licence cost and technical debt history. It is **optional** but enabled by
default when hardware resources permit.

---

## 1 Design Goals

| ID     | Goal                        | Why it matters                                               |
| ------ | --------------------------- | ------------------------------------------------------------ |
| RLC‑01 | **Model‑agnostic**          | Any RL algorithm (PPO, DQN, MCTS) can be hot‑swapped.        |
| RLC‑02 | **Data‑reuse**              | Leverage historical project telemetry and ontology stats.    |
| RLC‑03 | **Offline‑friendly**        | Train updates can run locally; remote fine‑tuning is opt‑in. |
| RLC‑04 | **Deterministic inference** | Same input plan → same value unless model retrained.         |
| RLC‑05 | **Explainability**          | Surface feature importance for each score.                   |

---

## 2 High‑Level Architecture

```mermaid
flowchart TD
  subgraph Critic Loop
    P[Planner output<br/>JSON plans] --> E(Feature Encoder)
    E --> V(Value Network)
    V --> O{Accept?}
  end
  O -->|Yes| BEST[Chosen Plan]
  O -->|No (try next)| P
```

- **Feature Encoder** converts each `PlanStep` to numeric vectors
  (licence_severity, estimated_time, dependency_count, etc.).
- **Value Network** predicts cumulative “cost” (lower is better).

---

## 3 Algorithm Choices

| Library           | Algo        | Pros                                        | Cons                         |
| ----------------- | ----------- | ------------------------------------------- | ---------------------------- |
| **Ray RLlib**     | PPO‑Clip    | Scales to multi‑core; built‑in checkpoints. | 400 MB Python dep footprint. |
| **border (Rust)** | PPO, DQN    | Native; compiles to WASM for plugin.        | Docs sparse; fewer algos.    |
| **scikit‑decide** | MCTS, LRTDP | Mix planning + RL in one lib.               | Python only.                 |

_Default_ = **border‑PPO** for small footprint; falls back to heuristic
scoring if crate not present.

---

## 4 Feature Vector Schema

| Feature                   | Type  | Range   |
| ------------------------- | ----- | ------- |
| `plan_len`                | int   | 1‑500   |
| `total_est_time`          | float | minutes |
| `gpl_violation_count`     | int   | 0‑N     |
| `confidence_rag`          | float | 0‑1     |
| `historical_success_rate` | float | 0‑1     |

Encoder lives at `crates/critic/src/encoder.rs`.

---

## 5 Training & Checkpoints

_Initial model_ shipped as
`~/.local/share/stack‑composer/critic/model.ckpt` (2 MB).  
Weekly cron retrains using the last 100 accepted plans:

```bash
just critic-train --episodes 200 --lr 3e-4
```

Checkpoints are signed (Ed25519) before loading.

---

## 6 Config (`critic.toml`)

```toml
enabled      = true
algo         = "border_ppo"
episodes     = 200
learning_rate = 3e-4
timeout_ms   = 500
```

CLI override: `--critic off` disables the subsystem.

---

## 7 Security & Sandboxing

- Training runs in an isolated task with
  `ulimit -m 1G` and CPU quota; no network by default.
- Checkpoints hashed and verified before inference.

---

## 8 Extension Points

| Hook           | Description                                                           |
| -------------- | --------------------------------------------------------------------- |
| `CriticImpl`   | Implement trait `fn value(Vec<Feature>) -> f32`.                      |
| WASI plugin    | Compile border model to Wasm and register as `critic.wasm`.           |
| Reward shaping | Provide custom reward fn via plugin to bias toward green energy, etc. |

---

## 9 Roadmap

| Version | Feature                                              |
| ------- | ---------------------------------------------------- |
| 0.8     | Border PPO with static checkpoint (this spec)        |
| 1.0     | Ray RLlib remote actor for distributed training      |
| 1.1     | Explainability panel (SHAP values)                   |
| 2.0     | Multi‑objective Pareto frontier & interactive tuning |

---

## 10 FAQ

**Is RL required?**  
No. If disabled, Stack Agent picks the plan with the lowest heuristic cost.

**How big is the model?**  
≈ 2 MB compressed checkpoint (32‑hidden‑neurons feed‑forward).

**Can I provide domain‑specific rewards?**  
Yes—write a WASI plugin implementing `reward(ctx) -> f32`.

---
